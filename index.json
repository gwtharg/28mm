[
{
	"uri": "https://gwtharg.github.io/28mm/paas/components_for_op/",
	"title": "Components  OP",
	"tags": ["openshift", "git", "nexus"],
	"description": "",
	"content": "Components for Operational Platform We are assuming that training delegates have prior knowledge of the following:\n Continuous Delivery  Jenkins  Nexus Repostory Manager  Postman  GitHub  GOCD  Continuous Integration  Gradle  Devops Mindset       Component Name Functionality Description Key Points Comments     Componet GitHub Repo Scource Code Management Source code of the component needs to be kept in GitHub with all its contents. Click here empty link? to understand how does the folder structure look like for the Microservices GitHub Repo Each Micro Service should have a GitHub Repo. You will have to setup GitHub Webhook  before Creating Jenkins Jobs   Jenkins Build Management You need to have BUILD Jobs for your component in Jenkins. Build job will compile your code and run UNIT and Component tests aginst it  See Jenkins Jobs  how to create Jenkins Build Jobs   Jenkins Release Management You need to have RELEASE Jobs for your component in Jenkins, Release jobs will do the following  See Jenkins Release  how to create Jenkins Release Jobs    gradle clean publishTestComponent \u0026ndash;refresh-dependencies Here we publish the \u0026ldquo;Integ\u0026rdquo; tests written by the owner of the components to Caboodle. This works for the caboodleTest folder      gradle clean build testAndBurn \u0026ndash;refresh-dependencies The following tasks takes place during this stages: 1. build docker 2. composeDevTestEnv (This is making call to the composer to deploy your component) 3. deployToDevTestEnv (check the deployment status) 4. Run Integration Tests 5. Delete the openshift project once the tests have passed.      gradle pushDockerImage caboodle:publish \u0026ndash;refresh-dependencies Publish the artefacts in the artefact repository     Chieftain Service Component  You need to make sure all the configuration details are stored in Chieftain.Click here empty link? to see how we create a new component in Chieftain using POSTMAN     Caboodle Artefacts 1. POM file 2. template.json 3. Docker Image Caboodle is our artefact repository which stores the following: 1. POM file for your component which contains all it\u0026rsquo;s dependencies in obps-maven repository. 2. zip file which contains \u0026ldquo;template.json file\u0026rdquo; - This file is used by composer to deploy the component on open shift. 3. Docker Image in obps-docker repository  Gradle publish command deploys all these artefacts in caboodle.Refer Gradle  file as a reference with all tasks   Route to Live Pipelines  All Components will should have a RTL JSON file which needs to be set-up in a release pipelines  trepo     Component_Test GitHub Repo  The purpose for which the component needs to be deployed should also be available in GitHub     Component_Test Jenkins Release Jobs build.gradle file with dependencies Gradle publish jobs to update caboodle with the POM file        Things you need to understand about the operational platform components\n Chieftain Caboodle Route to Live Pipelines Migration Strategy of Microservices  "
},
{
	"uri": "https://gwtharg.github.io/28mm/site_documentation/hugo_install/",
	"title": "Hugo Simple Install",
	"tags": ["hugo", "install"],
	"description": "",
	"content": "Install Hugo If you wish to try Hugo on your desktop then the simple instructions below will point you to a single install file.\nThe following note is from gohugo.io  which has more information on Hugo.\n Install Hugo on macOS, Windows, Linux, OpenBSD, FreeBSD, and on any machine where the Go compiler tool chain can run.\n There is lots of talk about “Hugo being written in Go”, but you don’t need to install Go to enjoy Hugo. Just grab a precompiled binary!\n Hugo is written in Go with support for multiple platforms. The latest release can be found at Hugo Releases  Hugo currently provides pre-built binaries for the following:\nmacOS (Darwin) for x64, i386, and ARM architectures Windows Linux OpenBSD FreeBSD Hugo may also be compiled from source wherever the Go toolchain can run; e.g., on other operating systems such as DragonFly BSD, OpenBSD, Plan 9, Solaris, and others. See Go Source  for the full set of supported combinations of target operating systems and compilation architectures.\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/caboodle_intro/",
	"title": "Caboodle",
	"tags": ["openshift", "git", "nexus"],
	"description": "",
	"content": "Using a Repository Manager A repository manager is a dedicated server application designed to manage repositories of binary components. The usage of a repository manager is considered an essential best practice for any significant usage.\nPurpose A repository manager serves these essential purposes:\n act as dedicated proxy server for public repositories provide repositories as a deployment destination for your project outputs  Benefits and Features Using a repository manager provides the following benefits and features:\n significantly reduced number of downloads off remote repositories, saving time and bandwidth resulting in increased build performance improved build stability due to reduced reliance on external repositories increased performance for interaction with remote SNAPSHOT repositories potential for control of consumed and provided artifacts creates a central storage and access to artifacts and meta data about them exposing build outputs to consumer such as other projects and developers, but also QA or operations teams or even customers provides an effective platform for exchanging binary artifacts within your organization and beyond without the need for building artifact from source  Available Repository Managers The following list (alphabetical order) of open source and commercial repository managers are known to support the repository format used. Please refer to the respective linked web sites for further information about repository management in general and the features provided by these products.\n  Apache Archiva  (open source)  JFrog Artifactory Open Source]  (open source)  JFrog Artifactory Pro  (commercial) Sonatype Nexus OSS (open source) Sonatype Nexus Pro (commercial)  Nexus 3.0  Taken from Sonatype Docs   Check out the blue bits for a brief understanding So what are components? A component is a resource like a library or a framework that is used as part of your software application at run-time, integration or unit test execution time or required as part of your build process. It could be an entire application or a static resource like an image.\nTypically these components are archives of a large variety of files including\n Java byte code in class files C object files text files e.g. properties files, XML files, JavaScript code, HTML, CSS binary files such as images, PDF files, sound and music files and many others  The archives are using numerous formats such as\n Java JAR, WAR, EAR formats plain ZIP or .tar.gz files Other package formats such as NuGet packages, Ruby gems, NPM packages Executable formats such as .exe or .sh files, Android APK files, various installer formats, …  Components can be composed of multiple, nested components themselves. E.g., consider a Java web application packaged as a WAR component. It contains a number of JAR components and a number of JavaScript libraries. All of these are standalone components in other contexts and happen to be included as part of the WAR component.\nComponents provide all the building blocks and features that allow a development team to create powerful applications by assembling them and adding their own business related components to create a full-fledged, powerful application.\nIn different tool-chains components are called artifact, package, bundle, archive and other terms. The concept and idea remains the same and component is used as the independent, generic term.\nComponents are identified by a set of specific values - the coordinates. A generic set of these coordinates is the usage of group, name and version. The names and the usage of these coordinates changes with the tool-chains used. Components can also be the anchor for further metadata.\nAssets Assets are the material addition to all this metadata. The actual archive file is an asset associated with the component. Many formats have a one-to-one mapping for component to asset.\nMore complex formats however have numerous assets associated with a component. For example a typical JAR component in a Maven repository is defined at least by the POM and the JAR files - both of which constitute separate assets belonging to the same components. Additional files such as JavaDoc or Sources JAR files are assets that belong to the same component.\nThe Docker format, on the other hand, gives assets unique identifiers and calls them Docker layers. These assets can be reused for different components - the Docker images. A Docker layer, for example, could be a specific operating system referenced by multiple Docker images.\nComponents in Repositories A wide variety of components exists and more are continuously created by the open source community as well as proprietary vendors. There are libraries and frameworks written in various languages on different platforms that are used for application development every day. It has become a default pattern to build applications by combining the features of multiple components with your own custom components containing your application code to create an application for a specific domain.\nIn order to ease the consumption and usage of components, they are aggregated into collections of components. These are called a repository and are typically available on the internet as a service. On different platforms terms such as registryand others are used for the same concept.\nExample for such repositories are\n the Central Repository, also known as Central the NuGet Gallery  RubyGems.org    npmjs.org    and a number of others. Components in these repositories are accessed by numerous tools including\n package managers like npm, nuget or gem, build tools such as Maven, Gradle, rake, grunt… IDE’s such as Eclipse, IntelliJ,…  and many, many others.\nRepositories have Formats The different repositories use different technologies to store and expose the components in them to client tools. This defines a repository format and as such is closely related to the tools interacting with the repository.\nE.g. the Maven repository format relies on a specific directory structure defined by the identifiers of the components and a number of XML formatted files for metadata. Component interaction is performed via plain HTTP commands and some additional custom interaction with the XML files.\nOther repository formats use databases for storage and REST API interactions, or different directory structures with format specific files for the metadata.\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/composer_info/",
	"title": "Composer Info",
	"tags": ["composer"],
	"description": "",
	"content": "REST enabled python based platform component which is used to deploy any component on to the operational platform. It follows the standard process of Getting the component name from the JSON file sent as a trigger\n Getting all dependencies of the component from the POM file present in caboodle Tagging all the components Getting configurations of all Compile dependencies from Chieftain Building a dynamic pipeline for GOCD to run Git push the dynamic pipeline to a GitHub repo which would run the pipelines in GOCD with three stages  build test Done    Build Stage Depending on the “Pipeline_Type” Composer does the following:Under the Elements Folder, looks for the folder with the same names as “Pipeline_Type” Executes the build.sh script for that element\nTest Stage At test stage, it is expceted to run test stage of the actual deployments. ex: if a component is deployed on the pod, we check the status of the pod to be active\nDone Stage This stage just monitors for the test stage to pass and passes with no actual execution in it.\nArchitectute Diagram: "
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/docker_automation/",
	"title": "Docker Automation",
	"tags": ["docker", "openshift", "gradle", "logstash"],
	"description": "",
	"content": "Logstash Example This document describes an example of the configuration and deployment of a Docker image in a RHEL environment, from a registry URL to the PaaS. It also goes over the CI/CD/Platform tools used in the process. Logstash is part of Centralised Logging on the platform, and is used to collect and manipulate system and application logs sent by fluentd log collectors  It resides on the PaaS and ships its logs to an external set of Elasticsearch VMs, currently in non-prod. This gives us a central way to manage and view large amounts of information in a quick fashion, using the Kibana front end. If you are interested have a read about the ELK stack  .\nThe files will match the layout of this working project  . There will be a note of what to change, should you want to build your own template.\nRequirements  Access to the non-prod OpenShift environment \u0026amp; relevant platform components (if you wish to replicate) Some understanding of: Elasticsearch (ELK stack), Red Hat Linux, Gradle, Jenkins, Docker, Python, OpenShift  Github layout We will use a standard for folder structure and Gradle commands to publish a copy of the image, assuming our project is called \u0026lsquo;myproject\u0026rsquo;\nmyproject/caboodle/ myproject/src/ myproject/src/docker/ myproject/src/templates/openshift/ Gradle files To save some space \u0026amp; time here, I\u0026rsquo;ll refer to a working example for these files and mention what to change:\n This file is our \u0026lsquo;root\u0026rsquo; build, it refers to build and publish tasks contained in other build.gradle files, base repos, and any GoCD version information, which we\u0026rsquo;ll ignore for now.  myproject/build.gradle\n This file describes our dependencies, registry information and publishing to Caboodle/Nexus. Under dependencies we will just set a dependency on openshift-project to demonstrate. Our Logstash instance will only build once an instance of openshift-project becomes available.  /Caboodle/build.gradle\n This file describes the docker build itself  /src/docker/Dockerfile\nOther config files  /openshift/template.json - How the OpenShift project should be configured, there are a few extra services configured here that can be ignored, but if building your own, replace references of \u0026lsquo;logstash\u0026rsquo; with your project name. It also contains Chieftain env variables which are explained later. myproject/settings.gradle - Defines our included submodules myproject/gradle.properties - Options to configure process used for the build. For a Docker image, change it to something unique (so the Caboodle Logstash isn\u0026rsquo;t overwritten), such as:  ... artifactName=myproject_logstash description=a test config of logstash ...  More info on the use of these two files  .\nDocker config  Dockerfile the config for building the image logstash.conf for the base Logstash config required for startup, containing fields replaced by the Python script logstash_entrypoint.py is the script to pull Logstash Chieftain config \u0026amp; replace config values pip.conf has the Caboodle URL for installing Python libraries requirements.txt for the actual Python modules  A Docker image is what Logstash will be built and run as our container, to be eventually deployed on OpenShift. The Gradle components are needed to actually build and publish artifacts (including our image), and now we need to configure the image:\nA rhel7 image is used to give us the ability to install Logstash and configure Python libraries required for configuration management. /src/docker/Dockerfile/ also refers to requirements.txt in the parent directory, pointing torequests and pyyaml for REST calls to Chieftain and creation of a .yml file for our Logstash instance respectively.\nLogstash is installed, followed by Python \u0026amp; the module installer pip, Java (Logstash needs 1.8) and the libraries defined in requirements.txt.\nInstall details Installation of Logstash and the necessary Python components\nRUN yum install -y logstash python2 python2-pip.noarch java \\ \u0026amp;\u0026amp; pip install -r /tmp/requirements.txt\nThe entrypoint is the default command to run when the Docker image starts, here we use the Python script itself. It is run to instrument the configuration of Logstash using our configuration data before the Logstash binary file is run, to ensure we have the most recent snapshot.\nENTRYPOINT [\u0026quot;python\u0026quot;,\u0026quot;/logstash_entrypoint.py\u0026quot;]\nLogstash requires a configuration file logstash.conf to start up, and it needs to point to a bunch of external Elastic non-prod VMs with some authentication, filtering and index config. The script will replace tagged fields in the local file with its Chieftain config, prior to copying it into the default Logstash config directory, and finally starting the Logstash executable that points to this config file.\nChieftain configuration If we want to build and test the image locally (who doesn\u0026rsquo;t), we need the Chieftain instance and token id, which is automatically generated as environment variables when we build using Composer for an openshift-project. CHIEFTAIN_INSTANCEID, CHIEFTAIN_TOKENID have been provided in the script in debug mode so can be replaced for testing, otherwise they\u0026rsquo;re fetched as environment variables.\nRefer to this working project  to create a unique service component in Chieftain; create an instance and then grab the id. Refer \u0026lsquo;Logstash\u0026rsquo; in Chieftain  as an example.\nBuilding To test locally, build the logstash image:\ndocker build -t myproject .\nTo run (interactively):\ndocker run -it myproject\nTesting If part of the build fails, it\u0026rsquo;s useful to run the container and investigate at that point, since we might back out too early (with a failed container) in later steps, e.g.\nStep 9 : COPY logstash_entrypoint.py / ---\u0026gt; Using cache ---\u0026gt; 3e14adcc988f docker run -it 3e14adcc988f\nOtherwise, executing a shell on the built image is good to see if everything\u0026rsquo;s in place, grabbing the container ID with the first command\ndocker ps\ndocker exec -it \u0026lt;container id\u0026gt; /bin/bash\nJenkins config Again the document here  is great for describing the CI build job that will upload the Logstash artifacts (including the Docker image) to Nexus. I recommend using it to create the job. This  is the Jenkins build used for the project as an example.\nComposer Next we need to Compose! This assumes we have:\n A complete \u0026amp; configured git layout A successful Jenkins build A Chieftain Logstash service component  Go to Composer  and type in the name of your service component, and search the group \u0026lsquo;docker\u0026rsquo; which was defined already in the build config. We can ignore tags for now (without a specific tag it will default to non-prod)\n Add a scale value of 1 Add an OpenShift project name \u0026lsquo;logstash-test\u0026rsquo;  The example json when \u0026lsquo;Compose\u0026rsquo; is selected should look like the below (versions may change). We are creating a new instance of openshift-project; if we want to deploy again in the same environment, we can reference an \u0026lsquo;existing\u0026rsquo; openshift_project - with tags \u0026lsquo;non-prod\u0026rsquo; in Chieftain.\nAfter a while Composer will give us a correlation ID, in the top right of the box, pasting it into Chieftain will provide us with details of the openshift-project and logstash instances now tied to it.\n{ \u0026#34;maven_name\u0026#34;: \u0026#34;com.bank.docker:logstash:0.0.45\u0026#34;, \u0026#34;scale\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;logstash\u0026#34;, \u0026#34;value\u0026#34;: 1 } ], \u0026#34;new_openshift_project_name\u0026#34;: \u0026#34;logstash-test\u0026#34; } GoCD In GoCD  we\u0026rsquo;ll hopefully see a full set of green, where the trigger is the call by composer, the OpenShift project is created, and finally Logstash is deployed.\nAnd in non-prod  OpenShift, the project which has successfully deployed!\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/elements/",
	"title": "Elements",
	"tags": ["openshift"],
	"description": "",
	"content": "Elements is where we store the difference parts of a request, an example of this is that a Redhat Virtual machine is an element of a build; by that we mean it is generally just a piece of a whole build and not the whole build itself and has creation and decommisioning scripts within its folder.\nThe layout of any element should follow this basic structure:\n src └── element_name ├── scripts │ ├── build.sh │ ├── decomm.sh │ ├── donedecomm.sh │ ├── done.sh │ ├── testdecomm.sh │ └── test.sh └── test └── unittest.sh  Generally we have all of our elements scripts as bash scripts but that isn\u0026rsquo;t always the case as we use python and powershell scripts in the same capacity so if you need to add a new element but want to use something other than shell then feel free! Just aslong as you have the same layout as above for building, testing and decommisioning. The reason for this is so that we can enforce that every element can be built safely and can be torn down safely.\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/jenkins_ci/",
	"title": "Jenkins CI",
	"tags": ["jenkins"],
	"description": "",
	"content": "Jenkins- Continuous Integration Server Jenkins is our open source Continuous Integration tool used for BUILD and Release Jobs.\nIt is an open source product with customisabel plugins which enables it to integrate it with multiple SCMs and run various types of build jobs. Each check in is verified with an automated build allowing early detection of problems. It has the ability to send the status of build and release jobs over an email or in HipChat rooms.\n ex: connectivity with git.mercurial,Subversion etc\n  ex: Jobs like; ant,Shell,Windows batch commands, gradle etc.\n Design Principle of Jenkins Jenkins is integrated with GitHUb and has been configured to run two ways:\n Build jobs for a specific branch Release jobs when there is a commit in the master branch  Build jobs for a specific branch This allows multiple developers to run build jobs for their specific branches.\nBranch Selector is a jenkins plugin installed for the same.\nAs part of the Build for microservices, we are running \u0026ldquo;gradle clean build command\u0026rdquo; which builds the docker image for the microservice and also runs UNIT and Components Tests for it.\nThis is followed by static analysis of the code and publishing unit tests results.\nRelease Jobs As part of the release jobs, apart from the build, Jenkins also runs the following jobs\n  gradle clean publishTestComponent \u0026ndash;refresh-dependencies - this is to publish the integration tests as artifacts in the Nexus repo\n  gradle clean build testAndBurn \u0026ndash;refresh-dependencies- as part of this job, after building the docker image, Call Composer- builds a dev environment and deploy all the components and then run integration test against the component.\n  gradle clean pushDockerImage caboodle:publish \u0026ndash;refresh-dependencies- As part of this job, the Image is published to caboodle(Artifact Repository)\n  \u0026ldquo;component_version=grep 'version' gradle.properties |awk -F= '{ print $2 }' | awk -F.0- '{ print $1 }'\n   component_version=2.0 tag_id=$component_version.$BUILD_NUMBER echo $tag_id git tag -a $tag_id -m \u0026quot;Jenkins Build\u0026quot; git push origin $tag_id\u0026quot; - This task is tagging GitHub back with the build number Other Tasks in Jenkins  Static Code Analysis using Sonarqube Dependency Check using oWASP JaCoCo Coverage Reports UNIT Tests Results  "
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/platform_bom/",
	"title": "Platform BOM",
	"tags": ["BOM", "gradle"],
	"description": "",
	"content": "This documents describes how to use the Platform BOM, it assumes the reader understands Semantic Version the Platform BOM and the services has been built using the Operational Platform Template.\nNormal The Platform BOM version is set by the variable \u0026ldquo;platformBomVersion\u0026rdquo; in the \u0026ldquo;gradle.properties\u0026rdquo; file. This should have a major version of greater than 3, for full Semantic Version. The default value is from auto-mate is currently \u0026ldquo;3+\u0026rdquo;, so it will pull the latest Platform BOM with major version 3.\nIt can be set to different levels:\n Specific version, for example 3.1.7, this will pull in a specific version of the Platform BOM. Minor version, for example 3.1+, this will pull in the latest Platform BOM with minor version 3.1, for example 3.1.5 or 3.1.3354 Major version, for example 3+, this will pull in the latest Platform BOM with major version 3, for example 3.2.5. This is the default. Latest, set to just +, this will pull in the latest including any major breaking changes  Even when not specifying a specific version, the specific version the services was build with will be recorded. For example if the \u0026ldquo;platformBomVersion\u0026rdquo; is 3+, and the latest published major version of the Platform BOM with major version 3 is 3.3.4; the service is built with the dependencies specified in this BOM. The built version can them be found from the information end point, as shown below. This version number can be used to either lock down the version, or for the HOTFIX process.\n{ \u0026quot;app\u0026quot;: { \u0026quot;platformBomVersion\u0026quot;: \u0026quot;3.1.7\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Canary Service\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;2.0.0-s528408-SNAPSHOT\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;canary-services\u0026quot; }, \u0026quot;framework\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;new-ms-template\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.1\u0026quot; }, \u0026quot;git\u0026quot;: { \u0026quot;commit\u0026quot;: { \u0026quot;time\u0026quot;: 1542830003000, \u0026quot;id\u0026quot;: \u0026quot;71742e7\u0026quot; }, \u0026quot;branch\u0026quot;: \u0026quot;master\u0026quot; }, \u0026quot;swaggerInfo\u0026quot;: { \u0026quot;swagger.uri\u0026quot;: \u0026quot;/about/swagger\u0026quot; } } Historial BOM Local SNAPSHOT While developing, may need to update or create and new version of a library, which needs to be tested in a service before it can be published for consumption.\nIf modify an existing library and require to test it in a services:\n Modify the library in question and publish the library to the local maven repository by running \u0026ldquo;gradle clean build install\u0026rdquo;. This will be a SNAPSHOT Version In the services either hard code the library in the build.gradle to be the SNAPSHOT version, or use the SNAPSHOT Platform BOM version. This is done by setting the \u0026ldquo;platformBomVersion\u0026rdquo; variable to 3.0.0-SNAPSHOT in the service.  If adding a new library it will be required to be added to the BOM, this is discussed in its instructions, as part of the the library release. So to test locally just add the dependency, hardcoded to SNAPSHOT.\nNOTE, do not check any code in with the dependency set to SNAPSHOT.\nHOTFIX When a service requires to be HOTFIX\u0026rsquo;d it may need to be built with the specific versions of the libraries it was originally built with. If the service has library semantic version (platformBomVersion \u0026gt;= 3), then this it is easy. If the service doesn\u0026rsquo;t it is possible but it easier to see if it can be converted to use Library Semantic Version.\nThe following sub-sections describe both scenarios.\nSEMANTIC VERSION If the issue is just with the service code then, given the version of the the service in question along with the Platform BOM version (obtained as described above)\n  Create a HOTFIX branch, by running the follow, which will create a branch called \u0026ldquo;x.y.z-HOTFIX\u0026rdquo; and check it out. Where \u0026ldquo;x.y.z\u0026rdquo; is the version of the service.\n./setup/create-hotfix.sh \u0026lt;service version\u0026gt;   Update the \u0026ldquo;platformBomVersion\u0026rdquo; in the \u0026ldquo;gradle.properties\u0026rdquo; file to the appropriate version.\n  Run the HOTFIX Jenkins job, which should be found under the \u0026ldquo;Application-CI-HOTFIX-Release\u0026rdquo; tab, against the HOTFIX branch.\n  Release the newly published HOTFIX artefact to the appropriate environment using the HOTFIX release process.\n  When the HOTFIX has been successfully released, the changes should be merged back into the appropriate main line branch (usually master).\n  If the issue is in the library then require to do a HOTFIX build of the library in question. Get the release tag of the library from the version of the Platform BOM identified above. Then similar to the above do the following.\n  Create a HOTFIX branch, by running the follow, which will create a branch called \u0026ldquo;x.y.z-HOTFIX\u0026rdquo; and check it out. Where \u0026ldquo;x.y.z\u0026rdquo; is the version of the platform BOM.\n./setup/create-hotfix.sh \u0026lt;service version\u0026gt;   Update the \u0026ldquo;platformBomVersion\u0026rdquo; in the \u0026ldquo;gradle.properties\u0026rdquo; file to the appropriate version.\n  Run the library HOTFIX Jenkins job, which can be found under the \u0026ldquo;Java-Common-Libraries-CI-Build\u0026rdquo; tab, against the HOTFIX branch.\n  Create a HOTFIX branch on the platform-bom GitHub repository, by running the script in the first set\n  In the platform-bom update the version for the library in question\n  Run the platform-bom HOTFIX Jenkins job, which can be found under the \u0026ldquo;Java-Common-Libraries-CI-Build\u0026rdquo; tab, against the HOTFIX branch. Taking a note of the HOTFIX platform BOM release tag.\n  Then apply the above steps to the services in question but use the HOTFIX BOM release tag for the plaformBOM version\n  As above, when the HOTFIX has been successfully released, the changes should be merged back into the appropriate main line branch (usually master).\n  LEGACY There are three options here all have associated risk and time costs. Each approach will need individual analysis not possible to document here.\n Use the latest SNAPSHOT BOM, if the old versions of the libraries aren\u0026rsquo;t required. If the versions required have a semantic version, move the code to use Library Semantic Version as described in the migration documents. This migration will have an additional time cost. Find the specific \u0026ldquo;sha\u0026rdquo; of the library at the time the service was built and do HOTFIX libraries from this point. This job is very manual and open to user mistakes.  "
},
{
	"uri": "https://gwtharg.github.io/28mm/git/version_strategy/",
	"title": "Versioning Strategy",
	"tags": ["git", "version"],
	"description": "",
	"content": "The document describes the versioning strategy for deliverable componenets.\nWhile we will be using semantic versioning and each release will be tagged with its semantic version, only major versions of each component will be supported in production.\nSo for each major version in production we will have a named branch. What we mean by \u0026lsquo;named branch\u0026rsquo; is a branch this is long lived, has forked of master and it will trigger a pipeline release on checkin. Apart from that they are normal branches.\nThe latest version of a component will be released from master branch as normal, but older major versions will be released from there own named branches.\nFor example, we have a component called example-service. In production we have two major version of example-service, major version 2 and major version 3. We will have two named branches, master for major version 3 and V2 for major version 2. All development for major version 3 will done on master, and all development for major version 2 will be done on the V2 branch. This can be extended to multiple major version. The latest major version will always be on master, while the older version will be on the Vn branches, where n is the major version number. The safest way is to think of each major version of a component as a totally separate component.\nNote, When a version is retired from production, after a suitable time, the associated Vn branch should be removed.\nThe only other specially named branch will be the hotfix branch, for fast release of productions defects, which will be described later.\nThe lifecycle of a code change is as follows\n A new branch is created from the approprite named branch at the appropriate point on the named branch. This will invariably be from the master branch at the head. Code is developed Code is checked into the development branch with an appropriate message. The message will be the JIRA number, one of the key words MAJOR, MINOR, PATCH, HOTFIX and a brief description. Note, if multiple commits are squished, be careful to include the highest key word in the commit message. Once development is complete then a pull request request is raised to merge into the parent named branch. The pull request will be reviewed and merged, if it passes review. If the code is merged, a job will be run against the branch, which will determine what version release tag should be applied. See section below for more detailed explication of the behaviour of this job.  NOTE: If a change requires to be done on more than one major version, the workflow for the changes should be treated as two seperate jobs, even if the changes are the same.\nIf a fast bug fix is required in production the workflow will be slightly different, as described below\n Determine version tag on component in production with the bug. Create a branch called hotfix, from the production tag. Develop fix on hotfix branch. Check in branch with message as above with HOTFIX keyword and create pull request The rest of the job is the same as above.  The major difference is the post fix managment. Once a bug is found the appropriate test scripts will need to modified to detect this bug and pass when bug is fixed. This means no version of component can pass through the pipeline until the bug is fixed. Once the hotfix is released, the fix should be merged in to each of the named branches affected by the bug.\nNote, while bug is used to describe the required change for the hotfix branch, is doesn\u0026rsquo;t necesarily need to be a code defect that needs to be released. It can be any change that is required to be released quickly, outside the normal development stream.\nChecking In Job When a change is checked/merged into a named branch a job will be run to tag the branch. The tag is a symanctic version number and is the release version. Note, even though this is a release number, this version of the code may not even get released as it won\u0026rsquo;t get through the pipeline.\nThis job will need to be developed.\nIt is possible for this job to fail and for the check in not to progress through the pipeline, but if it succeds the component will move to the next stage in the pipeline, which currently is the release Jenkins job.\nThis job will first look for the last tag. If no previous tag exisits the jobs will fail. If no tag exists a manual tag will need to added, this tag should be appropriate for the branch. Do we need to have a task to create a tag on repo creation?\nThe job can be re-run after any failure.\nOnce the last tag is found, the jobs looks at all the check in messages and finds the highest keyword. If no check in messages has a keyword the jobs will fail, and a checkin with an appropriate message will be required. On a Vn branch if the highest keyword in MAJOR the job will fail. On the hotfix branch if the highest keyword is not PATCH then the job will fail.\nOnce the highest keyword is determined, the job will incrment the previously found tag. If the keyword is MAJOR the major version in the tag is incremented. MINOR for minor and PATCH for patch version. The only exception to this pattern is on the hot fix branch where hotfix is added to the tag along with a build number, as the fix may not work first time. The branch is then tagged with this new tag, and the next pipeline jobs is started.\nThis table illustrates the incrementation of the tag\n   Start Tag Keyword New Tag     2.2.1 MAJOR 3.0.0   2.2.1 MINOR 2.3.0   2.2.1 PATCH 2.2.2   2.2.1 HOTFIX (on hotfix) 2.2.1-HOTFIX.n    Note: we have a hotfix number as it may take multiple attempts to fix the bug.\nCheck In To allow the above job to run the check in message will need to have a particular format, as described below\nJIRA-NO KEYWORD Check in description\nJIRA-NO The JIRA number for the change, all one word\nKEYWORD One of the values MAJOR, MINOR, PATCH or HOTFIX\nThen a breif description.\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/api-gateway/",
	"title": "What is an API-Gateway",
	"tags": ["composer"],
	"description": "",
	"content": "API gateway (replacement for DataPower) which front ends all the micro services, so any request coming in first hits API gateway \u0026amp; then API gateway works out to which microservice the request has to be sent. The following are the main responsibilities of API gateway.\n Look up the URI in the service cache to figuer out which service supports the requested endpoint Validate the request coming in against the swagger document of the service Authenticate/Authorize Forward the request to the relevant service  API gateway uses zuul for request routing \u0026amp; kubernates APIs for service discovery.\nHighlevel design Following diagram shows the highlevel design of API gateway (API Gateway) along with the micro service where the request is routed by API gateway (API Gateway)\nAPI gateway filters API gateway comprises of various filters which intercepts the request coming into API gateway (Pre filter) \u0026amp; response going out of API gatewayi (Post filter). Following diagram shows the API gateway pre/post filetrs\nService Discovery/Caching During API gateway service start up it discovers all the services available with the help of kubernates discovery client \u0026amp; then it queries every service for its swagger URI by calling endpoint /about/info. Once it gets the swagger URI, it then calls that endpoint to rerieve the swagger document of the service. The swagger document is then stored in the service cache against the service it belongs to. Once the API gateway is ready to serve requests, it looks up every request url in the service cache to find the service that supports the endpoint being requested \u0026amp; then routes the request to that service.\nFollowing diagram shows the service discovery flow\nDeploying a service behind API gateway To deploy a service that API gateway will route to there are specific endpoints that the service has to expose. Details of these can be found in the my copy is blank so awaiting document for this link Microservice Framework section. The service can be implemented in any language as long as the required endpoints are exposed. The important ones are:\n   Endpoint Description     /metadata/swagger Returns a swagger document describing the endpoints available within the service. Any request and response schema\u0026rsquo;s provided will be used for validation.   /metadata/mapping Returns a list of attributes and where to find them that can be used for creating an xacml request.   /about/health Returns whether or not the service is ready to service traffic    References  Solution Design - Gateway  spring-cloud-kubernetes  Netflix OSS, Spring and Kubernetes  "
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/what_is_docker/",
	"title": "What is Docker?",
	"tags": ["docker", "openshift"],
	"description": "",
	"content": "Docker is an Open Source software development platform. It\u0026rsquo;s main benefit is to package applications into \u0026ldquo;containers\u0026rdquo;, allowing them to be portable and lightweight among any system running the Linux OS. Container technology has been around for a few years, with Solaris Zones, IBM Wpars etc, but the hype around Dockers approach to containers has pushed this technology to the forefront in the last couple of years.\nBenefits of Docker Containers Scalability Container technology allows for a much larger scale of applications in virtualised environment. With a perfectly tuned container, you can have 4-6 times the number of application instances as you can using VM\u0026rsquo;s on the equivalent hardware. This means you reducde the overhead of running a VM, leaving you a small container running your application.\nPortability Containers elimates the \u0026ldquo;it works ok on my machine\u0026rdquo; problem as the same docker container image will run across all Linux based OS platforms.\nSpeed of development Developers can quickly develop, test and deploy docker containers. They don\u0026rsquo;t need to worry about setting up complex development environments.\nCan be pre-packaged Application providers are now starting to ship their applications in pre-built containers. Gone are the days of building an environemnt then having to install your application, it comes already packaged for you.\nFor example, let\u0026rsquo;s dowmload a pre-packaged HTTP server from redhat and launch the docker image:\n docker pull rhscl/httpd-24-rhel7\n  Using default tag: latest Trying to pull repository registry.access.redhat.com/rhscl/httpd-24-rhel7 ... Pulling from registry.access.redhat.com/rhscl/httpd-24-rhel7 4e5a7647df47: Pull complete 0001a3087112: Pull complete 5027fa50d337: Pull complete 503dde528ea5: Pull complete Status: Downloaded newer image for registry.access.redhat.com/rhscl/httpd-24-rhel7:latest  docker run -d -p 8080:8080 registry.access.redhat.com/rhscl/httpd-24-rhel7\n Now navigate to your machines hostname/ip and use port 8080 and voila:\nHow to build a docker image The easiest way to build a docker image is to use an existing image and layer your changes on top of it. This then creates a newer version with your updates applied. Let\u0026rsquo;s take the above pre-packaged http server as an example.\nWe are going to place a simple Hello World index page into the image to display when you access the URL\nTo do this, we create a Dockerfile. this file tells the docker build process what to do. So in our case, we are going to say where the original docker image should come FROM and then COPY in our new index file:\n cat Dockerfile\n FROM registry.access.redhat.com/rhscl/httpd-24-rhel7 COPY index.html /opt/rh/httpd24/root/var/www/html/  cat index.html\n \u0026lt;P\u0026gt;Hello world Once we have our Dockerfile in place, we tell docker to build our image, and give it a new name:\n docker build -t new_http_image .\n Sending build context to Docker daemon 3.072 kB Step 1 : FROM registry.access.redhat.com/rhscl/httpd-24-rhel7 ---\u0026gt; 6bf22bf8b187 Step 2 : COPY index.html /opt/rh/httpd24/root/var/www/html/ ---\u0026gt; d23ebab73188 Removing intermediate container 625cd8470385 Successfully built d23ebab73188 Now that we have our new image, we can execute it:\n docker run -p 8080:8080 new_http_image\n Now navigate to your machines hostname/ip and use port 8080 and voila:\nAs we use Open Shift to deploy docker containers in CYB, please refer to the\nOpenShift OpenShift Documentation  for how to deploy a docker image in OpenShift\nReferences Docker.com  Docker on Wikipedia  "
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/openshift/",
	"title": "OpenShift Container Platform",
	"tags": ["docker", "openshift", "kubernetes"],
	"description": "",
	"content": "Openshift is a Platform as a Service (PaaS) technology from Redhat based on Docker and Kubernetes.\nOpenshift provides the ability to orchistrate docker containers, providing request routing, health checks and automated scaling amongst other things.\nArchitecture Openshift is made up of multiple components that together orchestrate containers and allow them to talk to one another. There are multiple PaaS\u0026rsquo; deployed within the bank each with it\u0026rsquo;s own purpose.\n   PaaS Name Purpose     Non Production This PaaS is used for running the non production workloads, this will be where development and test purposes are deployed.   Production This PaaS is used to run production workloads which provide services to the customer.   Shared Services This PaaS is used to run the Operational Platform applications that are shared between Production and NonProduction PaaS\u0026rsquo;s. This is designed to run workloads that do not provide functionality to a customer but used to manage the Platform. This will run things like Composer, Kit CD services, Caboodle and Chieftain.    Openshift/Kubernetes Important Objects There are only a few important components that need to be understood to get started with Openshift.\nAll Openshift objects can be viewed and edited either in the UI or using the command line tool oc which is available from github.com (this will be packaged up eventually :-))\noc get ${object} will return the objects that you have access to.\nThis table provides a quick intro to some of the objects, further info can be found at the online documentation for Openshift and Kubernetes\n   Object oc get type (shortcut) Description     Project projects An area where a user can create and configure objects.   Namespace namespaces(ns) An extention of the project object, the namespace is a section of the SDN which is logically seperated from other namespaces.   Image Streams imagestreams (is) An object which is linked to a docker image deployed in the local docker registry of the PaaS. Other objects will refer to the imagestream rather than the docker images themselves. The image stream will track updates which can be used as a trigger for running deploys. Image streams also have tags similar to docker images (but they do not have to match the docker image tag)   Pod pods(po) A single deployable component which can be made up of 1 or more docker containers. This allows multiple docker containers to share a memory space, filesystem and network where there are tight dependencies.   Service services (svc) This is equivalent of a DNS entry and a load balancer when there are multiple instances of a pod running. This is only resolvable from containers running within the SDN of the PaaS.   Route routes This is the object that is used to configure the HA proxy\u0026rsquo;s when access is required to a service from outside of the PaaS.   Deployment Config deploymentconfigs(dc) Deployment configs are used to define how a docker image referenced by an imagestream should be deployed.   Replication Controllers replicationcontrollers(rc) A deployment config will create a replication controller rather than a pod. It is the replication controllers responsibility to create the pods, montior the health of the pods and start new pods if there are not enough pods running.   Persistent Volumes persistentvolumes(pv) These are filesystems which are shared amongst the nodes which can be mounted into a Pod when the docker container requests a Volume. This is a cluster level object whith various properties including size and write mode i.e. readWriteOnce(only a single instance of a pod can mount), readWriteMany(multiple instances of pods can mount), readOnly.   Persistent Volume Claim persistentvolumeclaims (pvc) These are used to allow a project to mark a persistent volume as used. A claim is really a request for a persistent volume which will then be matched and bound to a free volume. Each volume can only be bound to a single claim.    Whilst working on the PaaS it is possible to use a template to create multiple objects at a time. This is what elements relies on to do deploy applications to the Paas.\nUnderlying Fabric of the PaaS The PaaS is constructed with multiple types of physical servers. There are OpenShift Masters which are used to run the components used to manage the PaaS and OpenShift Nodes which are used to run the Pod workload.\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/pe_background/",
	"title": "Platform Engineering Background",
	"tags": ["openshift", "html"],
	"description": "",
	"content": " This is an embedded html page   \u0026nbsp;\nSome Background When the Platform Engineering team was formed there were a number of different processes in place for requesting work from the team. These included:\nProcess/MethodType of requestREAP Process / EPM\nPlanned project requests - Low detail and uncertain timingsEmailsUnplanned requests\nService Requests\nIncidents\nOCSMS Teams / HipchatWalkupService NowService Requests\nIncident Records\nProblem Records\nJiraPlanned Requests\nUnplanned Requests\nIncidents\nRTCIncidents\nUnplanned Requests\nSharepointUnplanned RequestsMeeting RequestsActions - should be plannedEscalationsUnplanned RequestsFollowing a review of the above the following requirements were captured:\nWhat is the team doing?How do we create headroom for the team?Minimise time spent managing work requestsClear prioritisation of tasks processHave good data about the work the team does to help decision makingPrimary tasks is to:\nRationalise work requests systems with defined SLAs\u0026nbsp;\n  "
},
{
	"uri": "https://gwtharg.github.io/28mm/git/branchingstrategy/",
	"title": "Branching and Merging Strategy",
	"tags": ["git"],
	"description": "",
	"content": "Most development is done on the internal Github but some code is shared with the external Bitbucket. In order to support this we have the same repo pushed to both GitHub and BitBucket.\nIt is best to consider GitHub and BitBucket as simply copies of the same git repository. There is no merging between GitHub and BitBucket, merging is done at the branch level and the same commit history is pushed to both systems.\nThere are slightly different strategies depending on what we are modifying.\nCommon Libraries Common libraries are strongly synchronised. All work for PaaS is done on a 2.0 branch and whenever a change is made on bitbucket/2.0 is must also be pushed to github/2.0 or vice versa.\nAs required the latest DYB changes on bitbucket/master branch will be merged into bitbucket/2.0 and pushed to github/2.0.\nNew Microservices This is the simplest scenario where a new repo is created on Github and development continues on github/master.\nOn a regular basis github/master is pushed to bitbucket/master so that in theory work could be done by people not on the network.\nExisting Microservices Initially bitbucket/master is pushed to github/master and a bitbucket/2.0 marker branch is created pointing to the same commit. Migration then continues on github/master.\nPeriodically we will have to synchronise any changes that have been made by DYB which involves merging bitbucket/master into github/master. The bitbucket/2.0 marker is moved to point to the merge commit and is pushed back to bitbucket.\nNote: Be very careful not to merge any PaaS work back into bitbucket/master since that belongs to DYB.\nAlexa - ScreenMedia ScreenMedia have their own git workflow and will periodically raise a pull request to merge their development branch into bitbucket/master. When this is merged it is pushed to github/master so that it can be built and deployed to the PaaS.\nNote: Since they are using a different workflow which uses a long lived development branch you should NOT delete their branch when merging. For the same reason do not rewrite history by rebasing or squashing during the merge, stick with standard merges.\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/site_documentation/markdown_notes/",
	"title": "Markdown Notes",
	"tags": ["markdown", "help"],
	"description": "",
	"content": "Tables You can create tables by assembling a list of words and dividing them with hyphens - (for the first row), and then separating each column with a pipe |:\nso this\n First Header | Second Header ------------ | ------------- Content from cell 1 | Content from cell 2 Content in the first column | Content in the second column Renders as this\n   First Header Second Header     Content from cell 1 Content from cell 2   Content in the first column Content in the second column    Headers # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 H1 H2 H3 H4 H5 H6 Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with **asterisks** or __underscores__. Strong emphasis, aka bold, with asterisks or underscores.\nStrikethrough uses two tildes. ~~Scratch this.~~ Strikethrough uses two tildes. Scratch this.\nOrdered Lists (In this example, leading and trailing spaces are shown with with dots: ⋅)\n1. First ordered list item 2. Another item 1. Actual numbers don't matter, just that it's a number 4. And another item.  First ordered list item Another item Actual numbers don\u0026rsquo;t matter, just that it\u0026rsquo;s a number And another item.  Unordered Lists To create an unordered list, add dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n* First item * * Second item * * Third item * * Fourth item * - First item - - Second item - - Third item - - Indent 1 `tab then -` - Indent 2 `tab then -` - Fourth item - + First item + + Second item + + Third item + + Fourth item +  First item * Second item * Third item * Fourth item *   First item - Second item - Third item -  Indent 1 tab then - Indent 2 Tab then -   Fourth item -   First item + Second item + Third item + Fourth item +  Lists with Tick boxes - [x] First item - [ ] Second item - [ ] Third item  First item Second item Third item  Line Breaks To create a line break \u0026lt;br\u0026gt;, end a line with two or more spaces, and then type return.\n So this would be a very long line that could with a line break now. [this is where the return was after two spaces] Now we have another line which will be after the line break. So this would be a very long line that could with a line break now. Now we have another line which will be after the line break.\nBlockquotes To create a blockquote, add a \u0026gt; in front of a paragraph.\n\u0026gt; So we should see this text in a block quote.  So we should see this text in a block quote.\n Horizontal Rules To create a horizontal rule, use three or more asterisks (***), dashes (\u0026mdash;), or underscores (___) on a line by themselves.\n*** --- ___    Links with names To create a link, enclose the link text in brackets (e.g., [Duck Duck Go]) and then follow it immediately with the URL in parentheses (e.g., (https://duckduckgo.com)).\nA better search engine would be [Duck Duck Go](https://duckduckgo.com) A better search engine would be Duck Duck Go\nRaw Links and Email Addresses To create a link or an email link , enclose the link text in angled brackets\nhttps://duckduckgo.com or yourname@home.com\n\u0026lt;https://duckduckgo.com\u0026gt; or \u0026lt;yourname@home.com\u0026gt; "
},
{
	"uri": "https://gwtharg.github.io/28mm/git/",
	"title": "GIT Info",
	"tags": ["git"],
	"description": "",
	"content": "Git Information Based on notes from old V: Drive from hVM\n  GIT Tagged files   Versioning Strategy   Branching and Merging Strategy   Semantic Version Enforcement   "
},
{
	"uri": "https://gwtharg.github.io/28mm/site_documentation/shortcode_test/",
	"title": "Shortcodes",
	"tags": ["hugo", "html", "markdown", "shortcodes"],
	"description": "",
	"content": " Due to some restrictions in displaying the rendered code please note that in the code snippets we show there is no leading or training spaces between the {{ and the \u0026lt; or % as shown in some of the descriptions.\n highlighting with shortcodes  {{\u0026lt; highlight go \u0026gt;}} Code to highlight here. {{\u0026lt; /highlight \u0026gt;}}\n Code to highlight here. button shortcode creates a button Changing the href to an external link works too.\n {{\u0026lt; button href=\u0026quot;/28mm/site_documentation/flowcharts/\u0026quot; \u0026gt;}}Site Content Info{{\u0026lt; /button \u0026gt;}}\n Site Content Info  rawhtml shortcode This allows us to enter html within a shortcode and run within a markdown file. We now can leverage html to enhance our markdown. To show the code here we had to used an Entity Encoder. Entity Encoder  This is the code and the rawhtml shortcode to allow html colours in our example.\n {{\u0026lt; rawhtml\u0026gt;}}\n \u0026lt;h2\u0026gt;HTML embedded in Markdown\u0026lt;/h2\u0026gt; Some text with \u0026lt;span style=\u0026quot;color:blue\u0026quot;\u0026gt; Simple \u0026lt;b\u0026gt;blue\u0026lt;/b\u0026gt; coloured text.\u0026lt;br\u0026gt; \u0026lt;/span\u0026gt;Back to normal text. Now we can highlight in \u0026lt;span style=\u0026quot;color:red\u0026quot;\u0026gt; \u0026lt;b\u0026gt;red\u0026lt;\u0026gt; /b\u0026gt; this is red.\u0026lt;/span\u0026gt;\u0026lt;br\u0026gt;  {{\u0026lt; /rawhtml \u0026gt;}}\n  HTML embedded in Markdown Some text with Simple blue coloured text.\nBack to normal text. Now we can highlight in red this is red.\nmermaid shortcode diagrams {{ \u0026lt; mermaid \u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] {{ \u0026lt; /mermaid \u0026gt;}}  graph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two]  More examples of mermaid can be found here. mermaid examples  expand shortcode   {{\u0026#x3C;expand\u0026#x3E; }} This is where we keep our additional expandable text. {{\u0026#x3C; /expand\u0026#x3E; }}     Expand me...   This is where we keep our additional expandable text.   notice note shortcode   {{ % notice note % }} Your notice goes here. {{ % /notice note % }}   Your notice goes here.\n "
},
{
	"uri": "https://gwtharg.github.io/28mm/",
	"title": "Auto-Doc",
	"tags": [""],
	"description": "",
	"content": "Auto-Doc Sample Front Matter Hugo Front Matter is the sites Metadata --- title: \u0026#34;You Title goes here\u0026#34; date: 2020-04-13T17:36:16+01:00 draft: true tags: [ \u0026#34;if required\u0026#34; ] categories: [ \u0026#34;Your category here\u0026#34; ] components: [\u0026#34;Jira aligned component\u0026#34;] authors: [\u0026#34;name of document owner\u0026#34;] --- A weight can be added to a page within a chapter. Similar to pinning a post. Also fields like tags: can be blank [\u0026quot;\u0026quot;] initially if you are unsure of content.\n Taxonomy Links  Categories in Hugo  Components in Hugo  Tags in Hugo  Authors in Hugo  "
},
{
	"uri": "https://gwtharg.github.io/28mm/paas/",
	"title": "Paas",
	"tags": ["openshift"],
	"description": "",
	"content": "Training Notes  Components OP   Caboodle   Composer Info   Docker Automation   Elements   Jenkins CI   Platform BOM   What is an API-Gateway   What is Docker?   OpenShift Container Platform   Platform Engineering Background   "
},
{
	"uri": "https://gwtharg.github.io/28mm/tfs/",
	"title": "TFS Internal",
	"tags": [""],
	"description": "",
	"content": "Imported via Pandoc Pages based on TFS documents  Team Types   "
},
{
	"uri": "https://gwtharg.github.io/28mm/authors/",
	"title": "Authors",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/bom/",
	"title": "BOM",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/bom/",
	"title": "BOM",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/caboodle/",
	"title": "caboodle",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/",
	"title": "Components",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/composer/",
	"title": "composer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/composer/",
	"title": "composer",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/authors/cybg/",
	"title": "cybg",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/docker/",
	"title": "docker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/docker/",
	"title": "docker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/git/",
	"title": "git",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/git/",
	"title": "git",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/gradle/",
	"title": "gradle",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/jenkins/",
	"title": "jenkins",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/jenkins/",
	"title": "jenkins",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/logstash/",
	"title": "logstash",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/nexus/",
	"title": "nexus",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/openshift/",
	"title": "openshift",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/openshift/",
	"title": "openshift",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/version/",
	"title": "version",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/document/",
	"title": "document",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/html/",
	"title": "html",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/hugo/",
	"title": "hugo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/categories/hugo/",
	"title": "hugo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/markdown/",
	"title": "markdown",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/shortcode/",
	"title": "shortcode",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/shortcodes/",
	"title": "shortcodes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/authors/walmo/",
	"title": "walmo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/go/",
	"title": "go",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/hugo/",
	"title": "hugo",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/install/",
	"title": "install",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/site_documentation/flowcharts/",
	"title": "Flowcharts Notes",
	"tags": ["help", "mermaid", "shortcodes"],
	"description": "",
	"content": "Here are some mermaid diagrams using shortcodes {{ \u0026lt; mermaid \u0026gt;}} graph LR; A[Hard edge] --\u0026gt;|Link text| B(Round edge) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result one] C --\u0026gt;|Two| E[Result two] {{ \u0026lt; /mermaid \u0026gt;}}  graph LR; A[Hard edge] --|Link text| B(Round edge) B -- C{Decision} C --|One| D[Result one] C --|Two| E[Result two]  Gantt Diagrams {{ \u0026lt; mermaid \u0026gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2020-01-06,2020-01-08 Active task :active, des2, 2020-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2020-01-06,24h Implement parser and json :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d {{ \u0026lt; /mermaid \u0026gt;}}  gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2020-01-06,2020-01-08 Active task :active, des2, 2020-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2020-01-06,24h Implement parser and json :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d  Class Diagrams {{ \u0026lt; mermaid \u0026gt;}} classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 \u0026lt;--\u0026gt; C2: Cool label {{ \u0026lt; /mermaid \u0026gt;}}  classDiagram Class01 C2 : Where am i? Class09 --* C3 Class09 --| Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08  C2: Cool label  Git Graphs {{ \u0026lt; mermaid \u0026gt;}} gitGraph: options { \u0026quot;nodeSpacing\u0026quot;: 60, \u0026quot;nodeRadius\u0026quot;: 8 } end commit branch walmo checkout walmo commit commit checkout master commit commit merge walmo {{ \u0026lt; /mermaid \u0026gt;}}  gitGraph: options { \"nodeSpacing\": 60, \"nodeRadius\": 8 } end commit branch walmo checkout walmo commit commit checkout master commit commit merge walmo  Single Line {{ \u0026lt; mermaid \u0026gt;}} graph LR; HTML--\u0026gt;Hugo; {{ \u0026lt; /mermaid \u0026gt;}}  graph LR; HTML--Hugo;  Adding Comments {{ \u0026lt; mermaid \u0026gt;}} graph TD A[Client] --\u0026gt;|tcp_443 \u0026amp; tcp_8080| B(F5 Load Balancer) B --\u0026gt;|tcp_443| C[Live Server] B --\u0026gt;|tcp_443| D[Live Server] {{ \u0026lt; /mermaid \u0026gt;}}  graph TD A[Client] --|tcp_443 \u0026 tcp_8080| B(F5 Load Balancer) B --|tcp_443| C[Live Server] B --|tcp_443| D[Live Server]  TB Diagram {{ \u0026lt; mermaid \u0026gt;}} graph TB c1--\u0026gt;a2 subgraph one a1--\u0026gt;a2 end subgraph two b1--\u0026gt;b2 end subgraph three c1--\u0026gt;c2 end {{ \u0026lt; /mermaid \u0026gt;}}  graph TB c1--a2 subgraph one a1--a2 end subgraph two b1--b2 end subgraph three c1--c2 end  Colours with style {{ \u0026lt; mermaid \u0026gt;}} id1(Start)--\u0026gt;id2(Stop) style id1 fill:#f00,stroke:#333,stroke-width:4px style id2 fill:#0f0,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5, 5 {{ \u0026lt; /mermaid \u0026gt;}}  graph LR id1(Start)--id2(Stop) style id1 fill:#f00,stroke:#333,stroke-width:4px style id2 fill:#0f0,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5, 5  Circles {{ \u0026lt; mermaid \u0026gt;}} graph LR id1((Drawing a circle)) {{ \u0026lt; /mermaid \u0026gt;}}  graph LR id1((Drawing a circle))  "
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/help/",
	"title": "help",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/mermaid/",
	"title": "mermaid",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/content/",
	"title": "content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/site_documentation/",
	"title": "Site Content Info",
	"tags": ["content", "help", "taxonomy"],
	"description": "",
	"content": "Basic help about the site   Our basic taxonomy comes from the Metadata in our \u0026lsquo;Front Matter\u0026rsquo;\nThis page has the following \u0026lsquo;Front Matter\u0026rsquo; taxonomy.\ntags: [\u0026quot;content\u0026quot; , \u0026quot;help\u0026quot; , \u0026quot;taxonomy\u0026quot;] catagories: [\u0026quot;help\u0026quot;] components: [\u0026quot;hugo\u0026quot;] authors: [\u0026quot;walmo\u0026quot;] Our types of taxonomy we are using are based on our configuration file.\nThe contents [\u0026ldquo;content\u0026rdquo; , \u0026ldquo;help\u0026rdquo; , \u0026ldquo;taxonomy\u0026rdquo;] we add when we create our file.\nMore accurate content in our Metadata will help users navigate the documents contained within the site.\n Hugo TOML YAML Mix config.toml uses TOML format:\ntitle = \u0026ldquo;My New Hugo Site\u0026rdquo;\nbut archetypes/default.md uses YAML\ntitle: \u0026ldquo;Test index\u0026rdquo;\nhttps://github.com/gohugoio/hugo/issues/5241\nThe reason is GitHub\u0026rsquo;s support for YAML front matter. Currently they do not support TOML front matter (Metadata).\nSo we are going to use **TOML** for the **config.toml** file then **YAML** for the rest of the contents.  Taxonomy Links  Categories in Hugo  Components in Hugo  Tags in Hugo  Authors in Hugo   Hugo Simple Install   Markdown Notes   Shortcodes   Flowcharts Notes   HTML Embedded Pages   PDF in HTML   Visio from Export   "
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/taxonomy/",
	"title": "taxonomy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/categories/paas/",
	"title": "paas",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/sdlc/",
	"title": "sdlc",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/git/semanticversionenforcement/",
	"title": "Semantic Version Enforcement",
	"tags": ["git"],
	"description": "",
	"content": "Going forward the commit comment format will be enforced in GitHub, to allow semantic versioning of the builds. This is a basic development discipline to allow control and automation of code delivery.\nThis enforcement will be rolled out sympathetically to all GitHub repositories, so as not to interfere with the in flight releases.\nThis change consists off the format of the commit comments, from which the semantic version is derived and set of scripts to enforce and help with commits.\nCommit Comment Format Development will be done on branches, created from the mainline streams. Currently only have the master mainline stream, but as process matures will have stream for each major semantic version.\nThere will be two types of branches, normal development and hotfix. Normal development will be branched from the head of the mainline stream (master), while hotfix will be branched from a tagged release. All work must be done from a JIRA ticket.\nFor a normal development release the commit comment should be of the format given by the following regex\n^[A-Z]+-[[:digit:]]+ : (MAJOR|MINOR|PATCH) : [[:alnum:]]+\nThe first part if the JIRA ticket id, the second part is the semantic change, the third part is a description of the change. For example\nDYB-12345 : MINOR : An example comment\nNote the spacing around field separator (:), this is to simplify reading, making it consistently formatted.\nThe semantic version part, should should take the value, MAJOR, MINOR or PATCH, all upper case; as explained below\n MAJOR : A change that is backward breaking, currently this isn\u0026rsquo;t supported MINOR : A change that adds new functionality PATCH : A fix for existing functionality  The decision on the semantic version value should be done by the developer, the ticket author and if required the appropriate technical consultant.\nFor a hotfix the commit message should be of the format given by the following regex\n^[A-Z]+-[[:digit:]]+ : HOTFIX : [[:alnum:]]+\nFor example\nDYB-54321 : HOTFIX : Need this fixed now\nThese formats should be applied to all commits, including when merging pull requests. Github isn\u0026rsquo;t helpful here, as it pre-fills the merge commit message, the title of the merge box; this should be edited manually when merging.\nDetermination of Semantic Version The reason for enforcement of the commit comment is to determine the semantic version of the generated artefact.\nFor a normal change, on a mainline stream, the version of the generated artefact will be major.minor.patch for example 3.4.12.\nA build which publishes an artefact will check the last published release, by looking for the the last tag. If none is found it will take a default value. It will then look at all the commit comments since that tag, and look for the highest semantic version value in all the commit comments (MAJOR \u0026gt; MINOR |\u0026gt; PATCH). It will increment the semantic version and tag the code with the new version.\nFor example if the last tag was 3.4.12 and the commit comments since then are\nPE-3567 : PATCH : A bugfix\nPE-3371 : PATCH : Fix logic\nPE-3371 : MINOR : Add new endpoint\nThe highest semantic version value is MINOR, so the semantic version will be incremented to 3.5.0\nSee the document about versioning strategy for a fuller description of this semantic version incrementing.\nFor a hotfix release the semantic version will take the format major.minor.patch-hotfix.buildnumberfor example 3.4.12-hotfix.22.\nWhen the release build is run against a hotfix branch, it will find the last tag, which will be the tag that the hotfix branch was created against. It will take this tag and add the term -hotfix and its build number to the tag and then tag the code with the new tag. The use of the original tag allows multiple hotfix developments, and the use of the build number allows multiple attempts at each of the hot fixes.\nSemantic Version Tools A new directory has been added to the ms-template, called setup, which contains the tools to control semantic versioning. This will also be added to all other template used by auto-mate.\nThe scripts in this directory includes the hooks to enforce the commit patterns and scripts to work out the semantic version from the commit comments. See the README in the directory for more details.\nThere are two script of interest for any development activities.\nThe first is development-setup.sh, this sets up the hooks on the developers local copy of the git project. This is called by default for every build but running\ngradle setupDevelopment\nWill do it without running a build.\nThe second is create-hotfix.sh , this creates the hotfix branch. It requires the release tag of the artefact to be hot fixed as its only argument.\nThe other scripts are described at the end of the README.\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/site_documentation/html_based_pages/",
	"title": "HTML Embedded  Pages",
	"tags": ["pdf", "html", "shortcode"],
	"description": "",
	"content": "  Trying colours You need to do this in html as markdown does not use colours by default.\n Some Markdown text with Simple blue coloured text.\nBack to normal text. Now we can highlight in red this is red.\n Below is the html for the colours    Some Markdown text with \u0026#x3C;span style=\u0026#x22;color:blue\u0026#x22;\u0026#x3E; Simple \u0026#x3C;b\u0026#x3E;blue\u0026#x3C;/b\u0026#x3E; coloured text.\u0026#x3C;br\u0026#x3E; \u0026#x3C;/span\u0026#x3E;Back to normal text. Now we can highlight in \u0026#x3C;span style=\u0026#x22;color:red\u0026#x22;\u0026#x3E; \u0026#x3C;b\u0026#x3E;red\u0026#x3C;/b\u0026#x3E; this is red.\u0026#x3C;/span\u0026#x3E;\u0026#x3C;br\u0026#x3E;   To show the code here we had to used an Entity Encoder. Entity Encoder  "
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/pdf/",
	"title": "pdf",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/shortcode/",
	"title": "shortcode",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/site_documentation/pdf_in_html/",
	"title": "PDF in HTML",
	"tags": ["pdf", "html", "shortcode"],
	"description": "",
	"content": "date: 2020-04-28T12:04:57+01:00 The html snippet is between our rawhtml shortcode.\n {{ \u0026lt; rawhtml \u0026gt; }} html here {{ \u0026lt; /rawhtml \u0026gt; }}\n   This is embedded html in a markdown file  The href is from localhost in this test   \u0026#x3C;object data=\u0026#x22;/28mm/site_documentation/image/twd.pdf\u0026#x22; type=\u0026#x22;application/pdf\u0026#x22; width=\u0026#x22;800px\u0026#x22; height=\u0026#x22;834px\u0026#x22;\u0026#x3E; \u0026#x3C;embed src=\u0026#x22;/28mm/site_documentation/image/twd.pdf\u0026#x22;\u0026#x3E; \u0026#x3C;p\u0026#x3E;This browser does not support PDFs. Please download the PDF to view it: \u0026#x3C;a href=\u0026#x22;http://localhost/28mm/site_documentation/image/twd.pdf\u0026#x22;\u0026#x3E;Download PDF\u0026#x3C;/a\u0026#x3E;.\u0026#x3C;/p\u0026#x3E; \u0026#x3C;/embed\u0026#x3E; \u0026#x3C;/object\u0026#x3E;   This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n   "
},
{
	"uri": "https://gwtharg.github.io/28mm/categories/help/",
	"title": "help",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/categories/help-document/",
	"title": "help, document",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/site_documentation/visio_from_export/",
	"title": "Visio from Export",
	"tags": ["visio, convert, help, pdf"],
	"description": "",
	"content": "This is a jpg export from Visio This is a png export from Visio  Using the rawhtml shortcode to embedded html   This is a Visio Drawing exported as a PDF  There is more user control over this type of file   \u0026#x3C;object data=\u0026#x22;/28mm/site_documentation/image/rts_pdf.pdf\u0026#x22; type=\u0026#x22;application/pdf\u0026#x22; width=\u0026#x22;800px\u0026#x22; height=\u0026#x22;834px\u0026#x22;\u0026#x3E; \u0026#x3C;embed src=\u0026#x22;/28mm/site_documentation/image/rts_pdf.pdf\u0026#x22;\u0026#x3E; \u0026#x3C;p\u0026#x3E;This browser does not support PDFs. Please download the PDF to view it: \u0026#x3C;a href=\u0026#x22;http://localhost/28mm/site_documentation/image/rts_pdf.pdf\u0026#x22;\u0026#x3E;Download PDF\u0026#x3C;/a\u0026#x3E;.\u0026#x3C;/p\u0026#x3E; \u0026#x3C;/embed\u0026#x3E; \u0026#x3C;/object\u0026#x3E;   This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n   "
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/visio-convert-help-pdf/",
	"title": "visio, convert, help, pdf",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/document/",
	"title": "document",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/help/",
	"title": "help",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/tfs/team_types/",
	"title": "Team Types",
	"tags": ["TFS", "document"],
	"description": "",
	"content": "Test snippet from document Aim of the Document This document is an attempt to guide TFS administrators through the process required to create a more complex hierarchical structure under the VM project folder.\nWe will run through an example using the IIB Technical Team to guide you through the process.\nInitial Considerations Most importantly is that we have had a discussion with the users to understand what it they wish to achieve. If teams require to view the results of queries from other teams \u0026amp; have the ability to show those on a common dashboard then this may be a reason to create a more complex structure. If the user just requires to be a member of many teams then this is not a valid reason to create a more complex structure.\nNote \u0026ndash; rights to view work/code/queries will be inherited with this model. So any team on a lower level must consent to this before creating any complex structure.\n"
},
{
	"uri": "https://gwtharg.github.io/28mm/tags/tfs/",
	"title": "TFS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gwtharg.github.io/28mm/components/tfs/",
	"title": "TFS",
	"tags": [],
	"description": "",
	"content": ""
}]